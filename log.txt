22Jan: Started "AI Engineering: Build, Train and Deploy Models with AWS SageMaker" by Patrik Szepesi in zerotomastery.io 
  - Puropose: to learn how to deply a machine learning model using hugging face in sagemaker for my music preference project
  - deployed a trained sentiment analysis model to get an idea of how huggingface and sagemaker work together (code is not included in this repo)

24Jan: Understanding mechanisms behind neural networks
  - how neural network combines different functions to fit a curve to datapoints(Statquest)
      -> backpropagation, gradient decent, activation functions

25-27Jan: Understanding LLM (DistilBert)
  -

28Jan: writing code for training notebook and script.py
  - fine tuned distilbert model:
      1. tokenize input sentence using distilbert tokenizer
      2. form embeddings of size 768 for each token
      3. pass the sequence into layers of transformers
      4. extract the output vector CLS token for sentence level representation (not trained explicitly but still yields good performance)
      5. pass through a linear layer 768->768
      6. apply ReLU to each neuron to give non linearity
      7. apply dropout (each neuron has 30% chance of turning to 0 and others are scaled up to avoid overfitting)
      8. pass the vector throguh another linear layer 768->4 where the outputs are 4 logits for the categories
  - create a custom class for the model structure (DistilBERTClass)
      -> inherits torch.nn.Module
      -> forward function defines the structure of the model
  - create a custom class for storing tokenized data (NewsDataset)
      -> allows creation of DataLoader which allowes batching and shuffling
      -> turns a row to df into a dictionary
      -> __getitem__ returns a dictionary of ids(tokenized inputs, tensor(max_len, )), mask(attention mask, tensor(512, )), and targets(category, single-element tensor)
  - create train and valid function
      -> for each patch of sequences: forward feed->calculate metrics->backpropagation->update parameters based on gradient
      -> loss function: cross entropy (how far off the prediction is from the target)
      -> model.eval(): evaluation mode ignores dropout layer
  - main funtion (flow of the code)
      1. parse arguments
      2. load the csv into pandas
      3. encode categories
      4. train test split
      5. create data loaders
      6. train the model my looping through number of epochs
      7. save the model and label encoder
  - final: completed the script but not yet debugged

29Jan: writing code for deployment notebook and inference.py
  - save label encoder in script.py as a part of the model using joblib

  - final: completed the script but not yet debugged

30Jan: writing smoke test code. debugging and training the models
  - model and input tensors must be on the same device -> e.g. ids.to(device)
  - item() only works on single element tensors. use tolist() otherwise
  - DistilBertTokenizer can return tokens in tensors for different frameworks, in my case return_tensor=pt
  - I can save configurations of the model instead of reconstructing the model in inference.py
  - label encoding before train test split can cause data leakage

  - final: training the sample (5% data) 
